{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yulyalr6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrEgRfDQecpn",
        "outputId": "71e00f4f-4b06-46a9-893a-e63c315cf27c"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.svm import SVC, NuSVC, LinearSVC, OneClassSVM, SVR, NuSVR, LinearSVR\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from nltk import WordPunctTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "%matplotlib inline \n",
        "sns.set(style=\"ticks\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdP13CXZedaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485b1649-039e-4349-bbb6-15e4441fffba"
      },
      "source": [
        "categories = [\"alt.atheism\", \"comp.graphics\", \"rec.autos\", \"sci.space\"]\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
        "data = newsgroups['data']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_yFbFjWeddL"
      },
      "source": [
        "def accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Вычисление метрики accuracy для каждого класса\n",
        "    y_true - истинные значения классов\n",
        "    y_pred - предсказанные значения классов\n",
        "    Возвращает словарь: ключ - метка класса, \n",
        "    значение - Accuracy для данного класса\n",
        "    \"\"\"\n",
        "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
        "    d = {'t': y_true, 'p': y_pred}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    # Метки классов\n",
        "    classes = np.unique(y_true)\n",
        "    # Результирующий словарь\n",
        "    res = dict()\n",
        "    # Перебор меток классов\n",
        "    for c in classes:\n",
        "        # отфильтруем данные, которые соответствуют \n",
        "        # текущей метке класса в истинных значениях\n",
        "        temp_data_flt = df[df['t']==c]\n",
        "        # расчет accuracy для заданной метки класса\n",
        "        temp_acc = accuracy_score(\n",
        "            temp_data_flt['t'].values, \n",
        "            temp_data_flt['p'].values)\n",
        "        # сохранение результата в словарь\n",
        "        res[c] = temp_acc\n",
        "    return res\n",
        "\n",
        "def print_accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Вывод метрики accuracy для каждого класса\n",
        "    \"\"\"\n",
        "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
        "    if len(accs)>0:\n",
        "        print('Метка \\t Accuracy')\n",
        "    for i in accs:\n",
        "        print('{} \\t {}'.format(i, accs[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmNUA8v0edg-",
        "outputId": "eb971d55-44b2-44e1-d1b0-4b61e87c130c"
      },
      "source": [
        "# CountVectorizer\n",
        "vocabVect = CountVectorizer()\n",
        "vocabVect.fit(data)\n",
        "corpusVocab = vocabVect.vocabulary_\n",
        "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество сформированных признаков - 34845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfzUfbbLedjX",
        "outputId": "bd92f38e-9d43-4e19-e099-5e7d2d10a52e"
      },
      "source": [
        "for i in list(corpusVocab)[1:10]:\n",
        "    print('{}={}'.format(i, corpusVocab[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertibles=10298\n",
            "from=14982\n",
            "bouton=7683\n",
            "gertrude=15453\n",
            "cms=9458\n",
            "udel=32340\n",
            "edu=12882\n",
            "katherine=19009\n",
            "reply=26923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U15wMcoGedlw",
        "outputId": "8aba159d-53bd-42ef-b969-88a5dee6fc74"
      },
      "source": [
        "test_features = vocabVect.transform(data)\n",
        "test_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2251x34845 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 338525 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30J_QbkwedoK",
        "outputId": "825a2595-77b1-4366-fedc-96f636af9360"
      },
      "source": [
        "# Размер нулевой строки\n",
        "len(test_features.todense()[0].getA1())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrDL955Hedqt",
        "outputId": "129be8df-644d-4157-95cc-38c4709161f2"
      },
      "source": [
        "# Непустые значения нулевой строки\n",
        "[i for i in test_features.todense()[0].getA1() if i>0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZdNBDyNAzUz",
        "outputId": "123b54e3-550d-4315-d679-d6e8faaeabbe"
      },
      "source": [
        "vocabVect.get_feature_names()[100:120]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['020021',\n",
              " '020259',\n",
              " '020356',\n",
              " '020359',\n",
              " '020504',\n",
              " '020637',\n",
              " '020701tan102',\n",
              " '020751',\n",
              " '02115',\n",
              " '02138',\n",
              " '02139',\n",
              " '02142',\n",
              " '02154',\n",
              " '021635',\n",
              " '021708',\n",
              " '02178',\n",
              " '0223',\n",
              " '022621tan102',\n",
              " '023044',\n",
              " '0233']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chQPCh_xAzYo"
      },
      "source": [
        "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
        "    for v in vectorizers_list:\n",
        "        for c in classifiers_list:\n",
        "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
        "            score = cross_val_score(pipeline1, newsgroups['data'], newsgroups['target'], scoring='accuracy', cv=3).mean()\n",
        "            print('Векторизация - {}'.format(v))\n",
        "            print('Модель для классификации - {}'.format(c))\n",
        "            print('Accuracy = {}'.format(score))\n",
        "            print('===========================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea6StqpQAzaP",
        "outputId": "aeb18366-b59d-4fe3-f460-c4200f8c36e2"
      },
      "source": [
        "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
        "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
        "VectorizeAndClassify(vectorizers_list, classifiers_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None,\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '000005102000': 5, '000021': 6,\n",
            "                            '000062david42': 7, '000100255pixel': 8,\n",
            "                            '000406': 9, '00041032': 10, '0004136': 11,\n",
            "                            '0004246': 12, '0004422': 13, '00044513': 14,\n",
            "                            '0004847546': 15, '0005': 16, '000601': 17,\n",
            "                            '0007': 18, '000710': 19, '00090711': 20,\n",
            "                            '000mi': 21, '000miles': 22, '000usd': 23,\n",
            "                            '0010580b': 24, '001125': 25, '0012': 26,\n",
            "                            '001200201pixel': 27, '001428': 28, '001555': 29, ...})\n",
            "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Accuracy = 0.9515810031069685\n",
            "===========================\n",
            "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None,\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '000005102000': 5, '000021': 6,\n",
            "                            '000062david42': 7, '000100255pixel': 8,\n",
            "                            '000406': 9, '00041032': 10, '0004136': 11,\n",
            "                            '0004246': 12, '0004422': 13, '00044513': 14,\n",
            "                            '0004847546': 15, '0005': 16, '000601': 17,\n",
            "                            '0007': 18, '000710': 19, '00090711': 20,\n",
            "                            '000mi': 21, '000miles': 22, '000usd': 23,\n",
            "                            '0010580b': 24, '001125': 25, '0012': 26,\n",
            "                            '001200201pixel': 27, '001428': 28, '001555': 29, ...})\n",
            "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0)\n",
            "Accuracy = 0.9586873797899097\n",
            "===========================\n",
            "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None,\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '000005102000': 5, '000021': 6,\n",
            "                            '000062david42': 7, '000100255pixel': 8,\n",
            "                            '000406': 9, '00041032': 10, '0004136': 11,\n",
            "                            '0004246': 12, '0004422': 13, '00044513': 14,\n",
            "                            '0004847546': 15, '0005': 16, '000601': 17,\n",
            "                            '0007': 18, '000710': 19, '00090711': 20,\n",
            "                            '000mi': 21, '000miles': 22, '000usd': 23,\n",
            "                            '0010580b': 24, '001125': 25, '0012': 26,\n",
            "                            '001200201pixel': 27, '001428': 28, '001555': 29, ...})\n",
            "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
            "                     weights='uniform')\n",
            "Accuracy = 0.6561479508803078\n",
            "===========================\n",
            "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use...\n",
            "                            '000000': 4, '000005102000': 5, '000021': 6,\n",
            "                            '000062david42': 7, '000100255pixel': 8,\n",
            "                            '000406': 9, '00041032': 10, '0004136': 11,\n",
            "                            '0004246': 12, '0004422': 13, '00044513': 14,\n",
            "                            '0004847546': 15, '0005': 16, '000601': 17,\n",
            "                            '0007': 18, '000710': 19, '00090711': 20,\n",
            "                            '000mi': 21, '000miles': 22, '000usd': 23,\n",
            "                            '0010580b': 24, '001125': 25, '0012': 26,\n",
            "                            '001200201pixel': 27, '001428': 28, '001555': 29, ...})\n",
            "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Accuracy = 0.9649054593874834\n",
            "===========================\n",
            "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use...\n",
            "                            '000000': 4, '000005102000': 5, '000021': 6,\n",
            "                            '000062david42': 7, '000100255pixel': 8,\n",
            "                            '000406': 9, '00041032': 10, '0004136': 11,\n",
            "                            '0004246': 12, '0004422': 13, '00044513': 14,\n",
            "                            '0004847546': 15, '0005': 16, '000601': 17,\n",
            "                            '0007': 18, '000710': 19, '00090711': 20,\n",
            "                            '000mi': 21, '000miles': 22, '000usd': 23,\n",
            "                            '0010580b': 24, '001125': 25, '0012': 26,\n",
            "                            '001200201pixel': 27, '001428': 28, '001555': 29, ...})\n",
            "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0)\n",
            "Accuracy = 0.9751223553780145\n",
            "===========================\n",
            "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use...\n",
            "                            '000000': 4, '000005102000': 5, '000021': 6,\n",
            "                            '000062david42': 7, '000100255pixel': 8,\n",
            "                            '000406': 9, '00041032': 10, '0004136': 11,\n",
            "                            '0004246': 12, '0004422': 13, '00044513': 14,\n",
            "                            '0004847546': 15, '0005': 16, '000601': 17,\n",
            "                            '0007': 18, '000710': 19, '00090711': 20,\n",
            "                            '000mi': 21, '000miles': 22, '000usd': 23,\n",
            "                            '0010580b': 24, '001125': 25, '0012': 26,\n",
            "                            '001200201pixel': 27, '001428': 28, '001555': 29, ...})\n",
            "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
            "                     weights='uniform')\n",
            "Accuracy = 0.8942737091285693\n",
            "===========================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A_gWi3dA_c7"
      },
      "source": [
        "# word2vec \n",
        "# Подготовим корпус\n",
        "corpus = []\n",
        "stop_words = stopwords.words('english')\n",
        "tok = WordPunctTokenizer()\n",
        "for line in newsgroups['data']:\n",
        "    line1 = line.strip().lower()\n",
        "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
        "    text_tok = tok.tokenize(line1)\n",
        "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
        "    corpus.append(text_tok1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSx3CLcmA_e2",
        "outputId": "8dc0ff5f-108f-4146-aca4-2c4f688d8b42"
      },
      "source": [
        "corpus[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['subject',\n",
              "  'convertibles',\n",
              "  'bouton',\n",
              "  'gertrude',\n",
              "  'cms',\n",
              "  'udel',\n",
              "  'edu',\n",
              "  'katherine',\n",
              "  'bouton',\n",
              "  'reply',\n",
              "  'bouton',\n",
              "  'gertrude',\n",
              "  'cms',\n",
              "  'udel',\n",
              "  'edu',\n",
              "  'organization',\n",
              "  'u',\n",
              "  'delaware',\n",
              "  'college',\n",
              "  'marine',\n",
              "  'studies',\n",
              "  'lewes',\n",
              "  'nntp',\n",
              "  'posting',\n",
              "  'host',\n",
              "  'gertrude',\n",
              "  'cms',\n",
              "  'udel',\n",
              "  'edu',\n",
              "  'lines',\n",
              "  'wondering',\n",
              "  'someone',\n",
              "  'could',\n",
              "  'point',\n",
              "  'somewhere',\n",
              "  'could',\n",
              "  'find',\n",
              "  'list',\n",
              "  'hopefully',\n",
              "  'comparison',\n",
              "  'convertibles',\n",
              "  'days',\n",
              "  'seems',\n",
              "  'like',\n",
              "  'making',\n",
              "  'big',\n",
              "  'comeback',\n",
              "  'sure',\n",
              "  'look'],\n",
              " ['jgreen',\n",
              "  'trumpet',\n",
              "  'calpoly',\n",
              "  'edu',\n",
              "  'james',\n",
              "  'thomas',\n",
              "  'green',\n",
              "  'subject',\n",
              "  'ussr',\n",
              "  'reached',\n",
              "  'moon',\n",
              "  'first',\n",
              "  'organization',\n",
              "  'california',\n",
              "  'polytechnic',\n",
              "  'state',\n",
              "  'university',\n",
              "  'san',\n",
              "  'luis',\n",
              "  'obispo',\n",
              "  'lines',\n",
              "  'suppose',\n",
              "  'soviets',\n",
              "  'managed',\n",
              "  'get',\n",
              "  'moon',\n",
              "  'rocket',\n",
              "  'working',\n",
              "  'made',\n",
              "  'first',\n",
              "  'could',\n",
              "  'beaten',\n",
              "  'us',\n",
              "  'either',\n",
              "  'rocket',\n",
              "  'blown',\n",
              "  'pad',\n",
              "  'thus',\n",
              "  'setting',\n",
              "  'back',\n",
              "  'saturn',\n",
              "  'v',\n",
              "  'went',\n",
              "  'boom',\n",
              "  'beaten',\n",
              "  'us',\n",
              "  'speculate',\n",
              "  'us',\n",
              "  'would',\n",
              "  'gone',\n",
              "  'head',\n",
              "  'done',\n",
              "  'landings',\n",
              "  'also',\n",
              "  'would',\n",
              "  'determined',\n",
              "  'set',\n",
              "  'base',\n",
              "  'earth',\n",
              "  'orbit',\n",
              "  'moon',\n",
              "  'whether',\n",
              "  'would',\n",
              "  'mars',\n",
              "  'would',\n",
              "  'depend',\n",
              "  'upon',\n",
              "  'whether',\n",
              "  'soviets',\n",
              "  'tried',\n",
              "  'go',\n",
              "  'setting',\n",
              "  'lunar',\n",
              "  'base',\n",
              "  'would',\n",
              "  'stretched',\n",
              "  'budgets',\n",
              "  'nations',\n",
              "  'think',\n",
              "  'military',\n",
              "  'value',\n",
              "  'lunar',\n",
              "  'base',\n",
              "  'would',\n",
              "  'outweigh',\n",
              "  'value',\n",
              "  'going',\n",
              "  'mars',\n",
              "  'least',\n",
              "  'short',\n",
              "  'run',\n",
              "  'thus',\n",
              "  'would',\n",
              "  'concentrated',\n",
              "  'moon',\n",
              "  'james',\n",
              "  'green',\n",
              "  'jgreen',\n",
              "  'oboe',\n",
              "  'calpoly',\n",
              "  'edu',\n",
              "  'believe',\n",
              "  'nation',\n",
              "  'commit',\n",
              "  'achieving',\n",
              "  'goal',\n",
              "  'decade',\n",
              "  'landing',\n",
              "  'man',\n",
              "  'moon',\n",
              "  'returning',\n",
              "  'safely',\n",
              "  'earth',\n",
              "  'john',\n",
              "  'f',\n",
              "  'kennedy',\n",
              "  'may'],\n",
              " ['sprec',\n",
              "  'j',\n",
              "  'acsu',\n",
              "  'buffalo',\n",
              "  'edu',\n",
              "  'joel',\n",
              "  'sprechman',\n",
              "  'subject',\n",
              "  'cleaning',\n",
              "  'eurowiper',\n",
              "  'boots',\n",
              "  'organization',\n",
              "  'ub',\n",
              "  'lines',\n",
              "  'nntp',\n",
              "  'posting',\n",
              "  'host',\n",
              "  'autarch',\n",
              "  'acsu',\n",
              "  'buffalo',\n",
              "  'edu',\n",
              "  'eurowiper',\n",
              "  'boots',\n",
              "  'white',\n",
              "  'throw',\n",
              "  'away',\n",
              "  'first',\n",
              "  'pair',\n",
              "  'since',\n",
              "  'found',\n",
              "  'way',\n",
              "  'cleaning',\n",
              "  'looked',\n",
              "  'almost',\n",
              "  'black',\n",
              "  'second',\n",
              "  'pair',\n",
              "  'white',\n",
              "  'ones',\n",
              "  'dirty',\n",
              "  'need',\n",
              "  'way',\n",
              "  'clean',\n",
              "  'w',\n",
              "  'removing',\n",
              "  'since',\n",
              "  'cut',\n",
              "  'remove',\n",
              "  'way',\n",
              "  'buy',\n",
              "  'black',\n",
              "  'ones',\n",
              "  'thanks',\n",
              "  'joel',\n",
              "  'joel',\n",
              "  'sprechman',\n",
              "  'sprec',\n",
              "  'j',\n",
              "  'acsu',\n",
              "  'buffalo',\n",
              "  'edu',\n",
              "  'university',\n",
              "  'buffalo',\n",
              "  'v',\n",
              "  'pff',\n",
              "  'ubvms',\n",
              "  'cc',\n",
              "  'buffalo',\n",
              "  'edu',\n",
              "  'black',\n",
              "  'white',\n",
              "  'thing',\n",
              "  'homosapian',\n",
              "  'thing',\n",
              "  'takes',\n",
              "  'big',\n",
              "  'man',\n",
              "  'cry',\n",
              "  'even',\n",
              "  'bigger',\n",
              "  'man',\n",
              "  'laugh',\n",
              "  'man',\n",
              "  'jack',\n",
              "  'handy'],\n",
              " ['ems',\n",
              "  'michael',\n",
              "  'apple',\n",
              "  'com',\n",
              "  'e',\n",
              "  'michael',\n",
              "  'smith',\n",
              "  'subject',\n",
              "  'hard',\n",
              "  'change',\n",
              "  'springs',\n",
              "  'f',\n",
              "  'truck',\n",
              "  'organization',\n",
              "  'circle',\n",
              "  'c',\n",
              "  'shellfish',\n",
              "  'ranch',\n",
              "  'shores',\n",
              "  'pacific',\n",
              "  'california',\n",
              "  'lines',\n",
              "  'bottom',\n",
              "  'line',\n",
              "  'worked',\n",
              "  'tips',\n",
              "  'techniques',\n",
              "  'included',\n",
              "  'article',\n",
              "  'c',\n",
              "  'zzpn',\n",
              "  'ax',\n",
              "  'constellation',\n",
              "  'ecn',\n",
              "  'uoknor',\n",
              "  'edu',\n",
              "  'callison',\n",
              "  'uokmax',\n",
              "  'ecn',\n",
              "  'uoknor',\n",
              "  'edu',\n",
              "  'james',\n",
              "  'p',\n",
              "  'callison',\n",
              "  'writes',\n",
              "  'article',\n",
              "  'apr',\n",
              "  'michael',\n",
              "  'apple',\n",
              "  'com',\n",
              "  'ems',\n",
              "  'michael',\n",
              "  'apple',\n",
              "  'com',\n",
              "  'e',\n",
              "  'michael',\n",
              "  'smith',\n",
              "  'writes',\n",
              "  'take',\n",
              "  'peculiar',\n",
              "  'tools',\n",
              "  'remove',\n",
              "  'rear',\n",
              "  'springs',\n",
              "  'ford',\n",
              "  'f',\n",
              "  'truck',\n",
              "  'x',\n",
              "  'leaf',\n",
              "  'springs',\n",
              "  'front',\n",
              "  'rear',\n",
              "  'big',\n",
              "  'socket',\n",
              "  'air',\n",
              "  'wrench',\n",
              "  'floor',\n",
              "  'jack',\n",
              "  'hydrolic',\n",
              "  'bottle',\n",
              "  'jack',\n",
              "  'home',\n",
              "  'found',\n",
              "  'needed',\n",
              "  'smaller',\n",
              "  'sockets',\n",
              "  'undo',\n",
              "  'shocks',\n",
              "  'wd',\n",
              "  'helped',\n",
              "  'sockets',\n",
              "  'needed',\n",
              "  'metric',\n",
              "  'exact',\n",
              "  'fit',\n",
              "  'able',\n",
              "  'use',\n",
              "  'sae',\n",
              "  'sockets',\n",
              "  'rather',\n",
              "  'close',\n",
              "  'mm',\n",
              "  'fun',\n",
              "  'canadian',\n",
              "  'ford',\n",
              "  'pictureing',\n",
              "  'undo',\n",
              "  'u',\n",
              "  'bolts',\n",
              "  'put',\n",
              "  'bottle',\n",
              "  'jack',\n",
              "  'axle',\n",
              "  'raise',\n",
              "  'bed',\n",
              "  'frame',\n",
              "  'take',\n",
              "  'stress',\n",
              "  'leaf',\n",
              "  'spring',\n",
              "  'undo',\n",
              "  'end',\n",
              "  'bolts',\n",
              "  'bushings',\n",
              "  'drop',\n",
              "  'spring',\n",
              "  'turn',\n",
              "  'bottom',\n",
              "  'leaf',\n",
              "  'connect',\n",
              "  'spring',\n",
              "  'bolts',\n",
              "  'sounds',\n",
              "  'right',\n",
              "  'undo',\n",
              "  'end',\n",
              "  'bolts',\n",
              "  'bushings',\n",
              "  'u',\n",
              "  'bolts',\n",
              "  'side',\n",
              "  'shock',\n",
              "  'absorber',\n",
              "  'jacking',\n",
              "  'frame',\n",
              "  'put',\n",
              "  'spare',\n",
              "  'tire',\n",
              "  'garage',\n",
              "  'floor',\n",
              "  'put',\n",
              "  'wooden',\n",
              "  'platform',\n",
              "  'top',\n",
              "  'get',\n",
              "  'floor',\n",
              "  'jack',\n",
              "  'high',\n",
              "  'enough',\n",
              "  'raise',\n",
              "  'frame',\n",
              "  'one',\n",
              "  'talll',\n",
              "  'truck',\n",
              "  'lifted',\n",
              "  'spring',\n",
              "  'free',\n",
              "  'axel',\n",
              "  'taking',\n",
              "  'block',\n",
              "  'gave',\n",
              "  'enough',\n",
              "  'room',\n",
              "  'undo',\n",
              "  'pin',\n",
              "  'holding',\n",
              "  'spring',\n",
              "  'pack',\n",
              "  'together',\n",
              "  'spring',\n",
              "  'pack',\n",
              "  'held',\n",
              "  'together',\n",
              "  'nut',\n",
              "  'top',\n",
              "  'round',\n",
              "  'head',\n",
              "  'end',\n",
              "  'wrench',\n",
              "  'head',\n",
              "  'vice',\n",
              "  'grips',\n",
              "  'worked',\n",
              "  'fine',\n",
              "  'soaked',\n",
              "  'nut',\n",
              "  'wd',\n",
              "  'came',\n",
              "  'right',\n",
              "  'flipped',\n",
              "  'bottom',\n",
              "  'spring',\n",
              "  'nightmare',\n",
              "  'waiting',\n",
              "  'happen',\n",
              "  'easy',\n",
              "  'though',\n",
              "  'physically',\n",
              "  'demanding',\n",
              "  'thing',\n",
              "  'well',\n",
              "  'easier',\n",
              "  'decent',\n",
              "  'trigger',\n",
              "  'job',\n",
              "  'ok',\n",
              "  'well',\n",
              "  'maybe',\n",
              "  'easy',\n",
              "  'terribly',\n",
              "  'difficult',\n",
              "  'donno',\n",
              "  'little',\n",
              "  'sore',\n",
              "  'today',\n",
              "  'working',\n",
              "  'foot',\n",
              "  'pipe',\n",
              "  'cheater',\n",
              "  'real',\n",
              "  'help',\n",
              "  'torque',\n",
              "  'spec',\n",
              "  'u',\n",
              "  'bolt',\n",
              "  'nuts',\n",
              "  'ft',\n",
              "  'lbs',\n",
              "  'kind',\n",
              "  'torque',\n",
              "  'spec',\n",
              "  'challenge',\n",
              "  'get',\n",
              "  'pack',\n",
              "  'bolt',\n",
              "  'back',\n",
              "  'spring',\n",
              "  'pack',\n",
              "  'squeeze',\n",
              "  'pack',\n",
              "  'two',\n",
              "  'hands',\n",
              "  'hold',\n",
              "  'bolt',\n",
              "  'third',\n",
              "  'put',\n",
              "  'nut',\n",
              "  'fourth',\n",
              "  'picking',\n",
              "  'wrench',\n",
              "  'vice',\n",
              "  'grips',\n",
              "  'fifth',\n",
              "  'sixth',\n",
              "  'hands',\n",
              "  'used',\n",
              "  'string',\n",
              "  'tie',\n",
              "  'pack',\n",
              "  'together',\n",
              "  'holding',\n",
              "  'pin',\n",
              "  'alignment',\n",
              "  'could',\n",
              "  'let',\n",
              "  'go',\n",
              "  'get',\n",
              "  'nut',\n",
              "  'wrench',\n",
              "  'vice',\n",
              "  'grips',\n",
              "  'getting',\n",
              "  'pin',\n",
              "  'back',\n",
              "  'lined',\n",
              "  'lift',\n",
              "  'block',\n",
              "  'challenge',\n",
              "  'discovered',\n",
              "  'axel',\n",
              "  'tilted',\n",
              "  'rolled',\n",
              "  'forward',\n",
              "  'one',\n",
              "  'drivers',\n",
              "  'side',\n",
              "  'bottle',\n",
              "  'jack',\n",
              "  'front',\n",
              "  'differential',\n",
              "  'tilted',\n",
              "  'back',\n",
              "  'line',\n",
              "  'enough',\n",
              "  'pin',\n",
              "  'head',\n",
              "  'drop',\n",
              "  'right',\n",
              "  'hole',\n",
              "  'passenger',\n",
              "  'side',\n",
              "  'wrestle',\n",
              "  'wheel',\n",
              "  'rolling',\n",
              "  'forward',\n",
              "  'inch',\n",
              "  'get',\n",
              "  'things',\n",
              "  'line',\n",
              "  'spent',\n",
              "  'hour',\n",
              "  'working',\n",
              "  'getting',\n",
              "  'pin',\n",
              "  'head',\n",
              "  'hole',\n",
              "  'lift',\n",
              "  'block',\n",
              "  'levers',\n",
              "  'ropes',\n",
              "  'impliments',\n",
              "  'distruction',\n",
              "  'took',\n",
              "  'break',\n",
              "  'thought',\n",
              "  'lined',\n",
              "  'anymore',\n",
              "  'since',\n",
              "  'lined',\n",
              "  'move',\n",
              "  'something',\n",
              "  'else',\n",
              "  'must',\n",
              "  'hmmm',\n",
              "  'axle',\n",
              "  'longer',\n",
              "  'constrained',\n",
              "  'move',\n",
              "  'hmmm',\n",
              "  'move',\n",
              "  'back',\n",
              "  'hmmm',\n",
              "  'much',\n",
              "  'easier',\n",
              "  'close',\n",
              "  'btw',\n",
              "  'ride',\n",
              "  'softer',\n",
              "  'quite',\n",
              "  'soft',\n",
              "  'hoping',\n",
              "  'least',\n",
              "  'sits',\n",
              "  'level',\n",
              "  'e',\n",
              "  'michael',\n",
              "  'smith',\n",
              "  'ems',\n",
              "  'apple',\n",
              "  'com',\n",
              "  'whatever',\n",
              "  'dream',\n",
              "  'begin',\n",
              "  'boldness',\n",
              "  'genius',\n",
              "  'power',\n",
              "  'magic',\n",
              "  'goethe',\n",
              "  'responsible',\n",
              "  'anyone',\n",
              "  'else',\n",
              "  'everything',\n",
              "  'disclaimed'],\n",
              " ['pes',\n",
              "  'hutcs',\n",
              "  'cs',\n",
              "  'hut',\n",
              "  'fi',\n",
              "  'pekka',\n",
              "  'siltanen',\n",
              "  'subject',\n",
              "  'detecting',\n",
              "  'double',\n",
              "  'points',\n",
              "  'bezier',\n",
              "  'curves',\n",
              "  'nntp',\n",
              "  'posting',\n",
              "  'host',\n",
              "  'hutcs',\n",
              "  'cs',\n",
              "  'hut',\n",
              "  'fi',\n",
              "  'organization',\n",
              "  'helsinki',\n",
              "  'university',\n",
              "  'technology',\n",
              "  'finland',\n",
              "  'lines',\n",
              "  'article',\n",
              "  'apr',\n",
              "  'kpc',\n",
              "  'com',\n",
              "  'jbulf',\n",
              "  'balsa',\n",
              "  'berkeley',\n",
              "  'edu',\n",
              "  'jeff',\n",
              "  'bulf',\n",
              "  'writes',\n",
              "  'article',\n",
              "  'ia',\n",
              "  'b',\n",
              "  'w',\n",
              "  'w',\n",
              "  'oeinck',\n",
              "  'waterland',\n",
              "  'wlink',\n",
              "  'nl',\n",
              "  'ferdinan',\n",
              "  'oeinck',\n",
              "  'waterland',\n",
              "  'wlink',\n",
              "  'nl',\n",
              "  'ferdinand',\n",
              "  'oeinck',\n",
              "  'writes',\n",
              "  'looking',\n",
              "  'information',\n",
              "  'detecting',\n",
              "  'calculating',\n",
              "  'double',\n",
              "  'point',\n",
              "  'cusp',\n",
              "  'bezier',\n",
              "  'curve',\n",
              "  'algorithm',\n",
              "  'literature',\n",
              "  'reference',\n",
              "  'mail',\n",
              "  'appreciated',\n",
              "  'useful',\n",
              "  'article',\n",
              "  'one',\n",
              "  'issues',\n",
              "  'transactions',\n",
              "  'graphics',\n",
              "  'believe',\n",
              "  'maureen',\n",
              "  'stone',\n",
              "  'one',\n",
              "  'authors',\n",
              "  'sorry',\n",
              "  'specific',\n",
              "  'reference',\n",
              "  'stone',\n",
              "  'derose',\n",
              "  'geometric',\n",
              "  'characterization',\n",
              "  'parametric',\n",
              "  'cubic',\n",
              "  'curves',\n",
              "  'acm',\n",
              "  'trans',\n",
              "  'graphics',\n",
              "  'manocha',\n",
              "  'canny',\n",
              "  'detecting',\n",
              "  'cusps',\n",
              "  'inflection',\n",
              "  'points',\n",
              "  'curves',\n",
              "  'computer',\n",
              "  'aided',\n",
              "  'geometric',\n",
              "  'design',\n",
              "  'pekka',\n",
              "  'siltanen']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77wJyzoCA_iK",
        "outputId": "9684626b-25be-440c-a863-5f6a87d2c65e"
      },
      "source": [
        "%time model_data = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7.12 s, sys: 75.1 ms, total: 7.19 s\n",
            "Wall time: 4.51 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMslTXwhA_ke"
      },
      "source": [
        "def sentiment(v, c):\n",
        "    model = Pipeline(\n",
        "        [(\"vectorizer\", v), \n",
        "         (\"classifier\", c)])\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print_accuracy_score_for_classes(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrI4tuBdA_mQ"
      },
      "source": [
        "class EmbeddingVectorizer(object):\n",
        "    '''\n",
        "    Для текста усредним вектора входящих в него слов\n",
        "    '''\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.size = model.vector_size\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([np.mean(\n",
        "            [self.model[w] for w in words if w in self.model] \n",
        "            or [np.zeros(self.size)], axis=0)\n",
        "            for words in X])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjzxFF20A_oW"
      },
      "source": [
        "# Обучающая и тестовая выборки\n",
        "boundary = 700\n",
        "X_train = corpus[:boundary] \n",
        "X_test = corpus[boundary:]\n",
        "y_train = newsgroups['target'][:boundary]\n",
        "y_test = newsgroups['target'][boundary:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiU-cDGSBTfK",
        "outputId": "a46f4591-b565-4717-bd5a-54b2366a95be"
      },
      "source": [
        "sentiment(EmbeddingVectorizer(model_data.wv), LogisticRegression(C=5.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Метка \t Accuracy\n",
            "0 \t 0.9365079365079365\n",
            "1 \t 0.8897058823529411\n",
            "2 \t 0.9176755447941889\n",
            "3 \t 0.8722891566265061\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}